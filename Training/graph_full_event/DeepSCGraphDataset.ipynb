{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27569fcf-8cd8-460c-b13f-60a020727b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /eos/user/r/rdfexp/ecal/cluster/output_deepcluster_dumper/windows_data/electrons/run3_126X_2023/ndjson_126X_mcRun3_2023_forPU65_byevent_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caedcaa-6ac1-4a7a-a915-17525943e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import gzip \n",
    "import json\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import uproot\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import InMemoryDataset,  Dataset\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch_geometric.data import Data\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import awkward as ak\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from torch_geometric.data import InMemoryDataset\n",
    "import os\n",
    "import glob\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a281dbe-effb-4e82-9da7-b79f1c72c730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d598f91-2350-43b4-b31d-d40f5a6c8435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def convert_to_tensor(features_dict):\n",
    "    # Convert dictionary of lists to a single tensor\n",
    "    features = torch.tensor([features_dict[key] for key in features_dict], dtype=torch.float).T\n",
    "    return features\n",
    "\n",
    "def create_data_object(nodes_features, nodes_sim_features, edges_idx, edges_labels):\n",
    "    # Convert nodes features and labels to tensors\n",
    "    x = convert_to_tensor(nodes_features)\n",
    "    y = convert_to_tensor(nodes_sim_features)\n",
    "    \n",
    "    # Convert edge indices and labels to tensors\n",
    "    edge_index = torch.tensor(edges_idx, dtype=torch.long).T\n",
    "    edge_attr = torch.tensor([edges_labels[key] for key in edges_labels], dtype=torch.float).T\n",
    "    \n",
    "    # Create a Data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3673f-b72e-4cb2-87e4-5643de8d8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECALGraphDataset(IterableDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None, graphs_in_file=512):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.pre_transform =  pre_transform\n",
    "        self.pre_filter = pre_filter\n",
    "        self.raw_dir = self.root + \"/raw\"\n",
    "        self.processed_dir = self.root+ \"/processed\"\n",
    "        self.graphs_in_file = graphs_in_file\n",
    "        self._nfiles = len(self.processed_files)\n",
    "        self._start = 0\n",
    "        self._end = self._nfiles\n",
    "        \n",
    "    @property\n",
    "    def raw_paths(self):\n",
    "        \"\"\"Returns expected raw files (not paths).\"\"\"\n",
    "        return [f  for f in glob.glob(os.path.join(self.root, \"raw\", \"*.tar.gz\"))]\n",
    "\n",
    "    @property\n",
    "    def processed_files(self):\n",
    "        \"\"\"Retrieve the list of processed .pt files.\"\"\"\n",
    "        processed_dir = os.path.join(self.root, \"processed\")\n",
    "        return sorted(f for f in os.listdir(processed_dir) if f.endswith('.pt'))\n",
    "\n",
    "    def _preprocess(self, args):\n",
    "        file, group_idx = args\n",
    "        dataset = read_file(file)\n",
    "        graphs_in_group = [ ]\n",
    "        ifiles = 0\n",
    "        for igraph, graph in enumerate(dataset):\n",
    "            # Create PyG data object\n",
    "            data = create_data_object(\n",
    "                nodes_features=graph['nodes_features'],\n",
    "                nodes_sim_features=graph['nodes_sim_features'],\n",
    "                edges_idx=graph['edges_idx'],\n",
    "                edges_labels=graph['edges_labels']\n",
    "            )\n",
    "            \n",
    "            # Apply pre-filter if specified\n",
    "            if self.pre_filter and not self.pre_filter(data):\n",
    "                continue\n",
    "                \n",
    "            # Apply pre-transform if specified\n",
    "            if self.pre_transform:\n",
    "                data = self.pre_transform(data)\n",
    "                \n",
    "            graphs_in_group.append(data)\n",
    "\n",
    "            if len(graphs_in_group) >= self.graphs_in_file or \\\n",
    "                   igraph == (len(dataset)-1):\n",
    "                print(f\"Saving file, {igraph}, {len(graphs_in_group)}\")\n",
    "                group_data_list = graphs_in_group\n",
    "                group_data, slices = torch_geometric.data.InMemoryDataset.collate(group_data_list)\n",
    "            \n",
    "                save_path = os.path.join(\n",
    "                    self.processed_dir,\n",
    "                    f'graph_data_group_{group_idx}_{ifiles}.pt')\n",
    "                torch.save((group_data, slices), save_path)\n",
    "                graphs_in_group = []\n",
    "                ifiles += 1\n",
    "            \n",
    "    \n",
    "    def preprocess(self, num_workers=4):\n",
    "        \"\"\"Processes raw data into PyTorch Geometric format.\"\"\"\n",
    "        \n",
    "        jobs = []\n",
    "        for ifile, file in enumerate(self.raw_paths):\n",
    "            jobs.append((file, ifile))\n",
    "\n",
    "        #p = Pool(num_workers)\n",
    "        #p.map(self._preprocess, jobs)\n",
    "        for job in jobs:\n",
    "            self._preprocess(job)\n",
    "\n",
    "    def _load_group(self, file_path):\n",
    "        \"\"\"Loads a group of graphs from a file and yields individual graphs.\"\"\"\n",
    "        full_path = os.path.join(self.root, \"processed\", file_path)\n",
    "        group_data, slices = torch.load(full_path)\n",
    "        \n",
    "        num_graphs = len(slices['x']) - 1\n",
    "        for i in range(num_graphs):\n",
    "            data = Data()\n",
    "            for key in group_data.keys():\n",
    "                if key in slices:\n",
    "                    start, end = slices[key][i], slices[key][i+1]\n",
    "                    if key == 'edge_index':\n",
    "                        data[key] = group_data[key][:, start:end]\n",
    "                    else:\n",
    "                        data[key] = group_data[key][start:end]\n",
    "                else:\n",
    "                    data[key] = group_data[key]\n",
    "            if self.transform:\n",
    "                data = self.transform(data)\n",
    "            yield data\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:  # single-process data loading, return the full iterator\n",
    "            iter_start = self._start\n",
    "            iter_end = self._end\n",
    "        else:  # in a worker process\n",
    "             # split workload\n",
    "            per_worker = int(math.ceil((self._end - self._start) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = self._start + worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, self._end)\n",
    "        for ifile in range(iter_start, iter_end):\n",
    "            yield from self._load_group(self.processed_files[ifile])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67cbc7-5b14-46a2-b121-3704dc23dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ECALGraphDataset(\"/eos/user/r/rdfexp/ecal/cluster/output_deepcluster_dumper/windows_data/gammas/run3_126X_2023_overlapTraining_double/ndjson_126X_mcRun3_2023_forPU65_byevent_v1/\", graphs_in_file=51200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1572fa7-b91f-4a8a-8b1c-216a5b6fd4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl =  DataLoader(\n",
    "        d,\n",
    "        batch_size=512,\n",
    "        num_workers=1,\n",
    "        pin_memory=True  # Faster data transfer to GPU\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d7884-897c-4381-bcda-c722c4453f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df  in enumerate(dl):\n",
    "    print(i, df)\n",
    "    if i == 100: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b8b18-7530-4fcf-af33-ebe22c2cac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "\n",
    "def create_batch_loader(dataset, batch_size=32, num_workers=0):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader instance for batching graphs.\n",
    "    \n",
    "    Args:\n",
    "        dataset: ECALLazyDataset instance\n",
    "        batch_size: Number of graphs per batch\n",
    "        shuffle: Whether to shuffle the dataset\n",
    "        num_workers: Number of worker processes for loading data\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader instance configured for graph batching\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "def create_dynamic_batch_loader(dataset, max_nodes=10000, mode='node', \n",
    "                              num_workers=0):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader instance with dynamic batching based on node count.\n",
    "    \n",
    "    Args:\n",
    "        dataset: ECALLazyDataset instance\n",
    "        max_nodes: Maximum number of nodes per batch\n",
    "        mode: Either 'node' or 'edge' for size measurement\n",
    "        shuffle: Whether to shuffle the dataset\n",
    "        num_workers: Number of worker processes for loading data\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader instance configured for dynamic batching\n",
    "    \"\"\"\n",
    "    from torch_geometric.loader import DynamicBatchSampler\n",
    "    \n",
    "    sampler = DynamicBatchSampler(\n",
    "        dataset,\n",
    "        max_num=max_nodes,\n",
    "        mode=mode,\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0cf44-9b50-484d-a0dc-844f5f05a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_batch_loader(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d42736-dc9c-4418-8332-dbd6f50c12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb97ac-d93f-4cbf-9783-141ddf224f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df  in enumerate(df):\n",
    "    print('.',end='')\n",
    "    if i == 1000: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b771c-c55b-43fd-bcfa-606cae195439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_graph(data):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with positions and sizes\n",
    "    for i, (ieta, iphi, en) in enumerate(zip(data.x[:, 4], data.x[:, 5], data.x[:, 0])):\n",
    "        G.add_node(i, pos=(ieta.item(), iphi.item()), size=en.item())\n",
    "    \n",
    "    # Add edges\n",
    "    for edge in data.edge_index.T:\n",
    "        G.add_edge(edge[0].item(), edge[1].item())\n",
    "    \n",
    "    # Get positions and sizes for drawing\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    sizes = [G.nodes[node]['size'] for node in G.nodes]  # Scale sizes for better visualization\n",
    "    \n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=sizes, node_color='skyblue', edge_color='gray', font_size=8) #node_size=sizes, \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02644d2-3257-45e8-95a9-61e8407f0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384917db-b9db-4cba-b161-afd12b0f0c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_json)):\n",
    "    graph_data = data_json[i]\n",
    "    g = create_data_object(graph_data[\"nodes_features\"], graph_data[\"nodes_sim_features\"], \n",
    "                   graph_data[\"edges_idx\"], graph_data[\"edges_labels\"])\n",
    "    draw_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13556d-7c69-4a59-a8a5-b708c4f879f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
